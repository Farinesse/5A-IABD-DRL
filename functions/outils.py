import io
import math
import os
import pickle
import secrets
import time

import keras
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from statistics import mean

from colorama import Fore
from tqdm import tqdm

@tf.function
def dqn_model_predict(model, s):
    """Pr√©diction des Q-valeurs pour un √©tat donn√©."""
    s = tf.ensure_shape(s, (None,))
    return model(tf.expand_dims(s, 0))[0]


def save_model(model, file_path):
    """
    Sauvegarde le mod√®le en utilisant Pickle.

    :param model: Le mod√®le √† sauvegarder
    :param file_path: Le chemin du fichier o√π sauvegarder le mod√®le
    """
    try:

        with open(file_path, 'wb') as f:
            pickle.dump(model, f)
        print(f"Mod√®le sauvegard√© avec succ√®s dans {file_path} au format Pickle")
    except Exception as e:
        print(f"Erreur lors de la sauvegarde du mod√®le : {e}")


def load_model_pkl(file_path):
    """
    Charge un mod√®le sauvegard√© avec Pickle.

    :param file_path: Le chemin du fichier o√π le mod√®le est sauvegard√©
    :return: Le mod√®le charg√©
    """
    try:
        with open(file_path, 'rb') as f:
            model = pickle.load(f)
        print(f"Mod√®le charg√© avec succ√®s √† partir de {file_path}")
        return model
    except Exception as e:
        print(f"Erreur lors du chargement du mod√®le : {e}")

def save_files(
        online_model,
        algo_name,
        results_df,
        env,
        num_episodes,
        gamma=None,
        alpha=None,
        start_epsilon=None,
        end_epsilon=None,
        update_target_steps=None,
        optimizer=None,
        save_path=None,
        memory_size=None,
        batch_size=None,
        custom_metrics = None
):
    if save_path is not None:
        if save_path.endswith(".pkl"):
            save_path = f'{save_path[:-4]}_{secrets.token_hex(4)}.pkl'
        else:
            save_path = f'{save_path}_{secrets.token_hex(4)}.pkl'

        dirn = save_path.replace(".pkl", "")

        if not os.path.exists(dirn):
            try:
                os.makedirs(dirn)
                print(f"Directory created: {dirn}")
            except OSError as e:
                print(f"Error creating directory {dirn}: {e}")
        else:
            print(f"Directory already exists: {dirn}")


        save_path = f'{dirn}/{save_path}'

        csv = f'{save_path}_metrics.csv'

        print(f"Saving model to {save_path}")
        save_model(online_model, save_path)

        print(f"Saving results to {csv}")
        results_df.to_csv(csv, index=False)

        print(f"Plotting training metrics to {csv}.png")
        plot_csv_data(
            csv,
            model = online_model,
            title = f"Training Metrics {algo_name} - {env.env_description()} - {save_path}",
            custom_dict = {
                "Episodes": num_episodes,
                "Gamma": gamma,
                "Alpha": alpha,
                "Start Epsilon": start_epsilon,
                "End Epsilon": end_epsilon,
                "Update Target Steps": update_target_steps,
                "Memory": memory_size,
                "Batch Size": batch_size,
                "Optimizer": optimizer.get_config()
            },
            algo_name = algo_name,
            env_descr = env.env_description()
        )

def logarithmic_decay(episode, start_epsilon, end_epsilon, decay_rate=0.01):
    return max(end_epsilon, start_epsilon - decay_rate * math.log(1 + episode))


def custom_two_phase_decay(episode, start_epsilon, end_epsilon, total_episodes, midpoint=0.5):
    # Point de transition entre phase lente et phase rapide
    transition_point = int(total_episodes * midpoint)

    if episode <= transition_point:
        # Premi√®re moiti√© : d√©croissance tr√®s lente (quasi lin√©aire)
        progress = episode / transition_point
        return start_epsilon - (start_epsilon - 0.5) * (progress ** 3)
    else:
        # Seconde moiti√© : d√©croissance plus rapide (exponentielle)
        remaining_episodes = total_episodes - transition_point
        progress = (episode - transition_point) / remaining_episodes
        return 0.5 * math.exp(-5 * progress)  # 0.5 est la valeur d'epsilon au point de transition


def epsilon_greedy_action(q_s: tf.Tensor,
                          mask: tf.Tensor,
                          available_actions: np.ndarray,
                          epsilon: float) -> int:
    if np.random.rand() < epsilon:
        return np.random.choice(available_actions)
    else:
        inverted_mask = tf.constant(1.0) - mask
        masked_q_s = q_s * mask + tf.float32.min * inverted_mask
        return int(tf.argmax(masked_q_s, axis=0))

def human_move(game):
    """
    Permet √† l'humain de choisir une case sur le plateau (de 0 √† 8).
    """
    valid_square = False
    val = None
    while not valid_square:
        square = input('Entrez un choix de case (0-8) : ')
        try:
            val = int(square)
            if val not in game.available_actions_ids():  # V√©rifie les actions disponibles
                raise ValueError
            valid_square = True
        except ValueError:
            print("Case invalide, essayez √† nouveau.")
    return val


def play(game, player_x, print_game=True):
    """
    Permet √† deux joueurs (humain ou IA) de jouer une partie de TicTacToe.
    """
    if print_game:
        game.display()

    letter = 'X'  # Le premier joueur commence avec 'X'
    while not game.is_game_over():

        square = player_x(game)
        game.step(square)  # Ex√©cute l'action
        print("state : ", game.state_description())
        print("ACTION : ", game.available_actions_ids())

        print("ACTION MASK : ", game.action_mask())

        if print_game:
            game.display()

        if game.is_game_over():
            if print_game:
                if game.score() == 1.0:
                    print(f'{letter} a gagn√©!')
                elif game.score() == 0.0:
                    print('C\'est un match nul!')
                else :
                    print(f'O a gagn√©!')
            return letter  # Retourne le gagnant ou None en cas de match nul

        # Changement de joueur
        letter = 'O' if letter == 'X' else 'X'


def human_move_line_world(game):
    """
    Permet √† l'humain de choisir une action dans LineWorld.
    """
    valid_action = False
    action = None
    while not valid_action:
        print("\nChoisissez une action :")
        print("0: Rester sur place")
        print("1: Aller √† gauche")
        print("2: Aller √† droite")
        try:
            action = int(input("Votre choix (0, 1, 2) : "))
            if action in game.available_actions_ids():  # V√©rifie les actions disponibles
                valid_action = True
            else:
                print("Action invalide ! Veuillez choisir parmi les actions disponibles.")
        except ValueError:
            print("Entr√©e invalide, veuillez entrer un nombre.")
    return action


def play_line_grid_world(game, player_human=None, player_random=None, print_game=True):
    """
    Permet √† un humain et un agent al√©atoire de jouer √† LineWorld.
    """
    if print_game:
        game.display()

    while not game.is_game_over():
        if player_human:
            action = player_human(game)
            game.step(action)
            if print_game:
                game.display()

            if game.is_game_over():
                if print_game:
                    print("Jeu termin√© ! L'agent est dans un √©tat terminal.")
                break

        if player_random:
            # Tour de l'agent al√©atoire
            action = player_random(game)
            game.step(action)
            if print_game:
                game.display()

            if game.is_game_over():
                if print_game:
                    print("Jeu termin√© ! L'agent est dans un √©tat terminal.")
                break


def human_move_grid_world(game):
    """
    Permet √† l'humain de choisir une action dans GridWorld.
    """
    valid_action = False
    action = None
    while not valid_action:
        print("\nChoisissez une action :")
        print("0: Aller en haut")
        print("1: Aller en bas")
        print("2: Aller √† gauche")
        print("3: Aller √† droite")
        try:
            action = int(input("Votre choix (0, 1, 2, 3) : "))
            if action in game.available_actions_ids():  # V√©rifie les actions disponibles
                valid_action = True
            else:
                print("Action invalide ! Veuillez choisir parmi les actions disponibles.")
        except ValueError:
            print("Entr√©e invalide, veuillez entrer un nombre.")
    return action


def play_grid_world(game, player_human, player_random, print_game=True):
    """
    Permet √† un humain et un agent al√©atoire de jouer √† GridWorld.
    """
    if print_game:
        game.display()

    while not game.is_game_over():
        # Tour de l'humain
        action = player_human(game)
        game.step(action)
        if print_game:
            game.display()

        if game.is_game_over():
            if print_game:
                print("Jeu termin√© ! L'agent est dans un √©tat terminal.")
            break

        # Tour de l'agent al√©atoire
        action = player_random(game)
        game.step(action)
        if print_game:
            game.display()

        if game.is_game_over():
            if print_game:
                print("Jeu termin√© ! L'agent est dans un √©tat terminal.")
            break


def play_game_manual():
    """Fonction pour jouer manuellement contre un adversaire al√©atoire."""
    env = None  # FarkleDQNEnv(num_players=2, target_score=5000)
    state, _ = env.reset()
    done = False

    while not env.is_game_over():
        # Affichage plus clair de l'√©tat du jeu
        print("\n" + "=" * 50)
        print("√âtat du jeu:")
        print(f"üé≤ D√©s actuels: {env.dice_roll}")
        print(f"üéØ Score du tour: {env.round_score}")
        print(f"üë• Scores des joueurs: {env.scores}")
        print(f"üéÆ Joueur actuel: {env.current_player + 1}")
        print(f"üé≤ D√©s restants: {env.remaining_dice}")
        print("=" * 50 + "\n")

        if env.current_player == 0:  # Tour du joueur humain
            # Affichage des actions valides
            print("\nActions valides disponibles:")
            valid_actions = env.available_actions_ids()
            for action_id in valid_actions:
                action_binary = format(action_id, '07b')
                print(f"\nID: {action_id}")
                print(f"Action binaire: {action_binary}")
                # Explication d√©taill√©e de l'action
                dice_selection = list(action_binary[:-1])
                stop_action = action_binary[-1]

                # Montrer quels d√©s seraient gard√©s
                kept_dice = []
                for i, (die, keep) in enumerate(zip(env.dice_roll, dice_selection)):
                    if keep == '1':
                        kept_dice.append(die)

                print(f"D√©s √† garder: {kept_dice}")
                print(f"Action stop: {'Oui' if stop_action == '1' else 'Non'}")

            # Saisie de l'action avec validation
            while True:
                try:
                    action = int(input("\nEntrez l'ID de votre action : "))
                    if action_id in valid_actions:
                        #    action = [int(b) for b in format(action_id, '07b')]
                        break
                    print("‚ùå Action invalide. Veuillez choisir parmi les actions list√©es.")
                except ValueError:
                    print("‚ùå Veuillez entrer un nombre valide.")

        else:  # Tour de l'adversaire al√©atoire
            print("\nTour de l'adversaire...")
            action = env.get_random_action()
            print(f"L'adversaire choisit : {action}")

        # Ex√©cution de l'action
        state, reward, done, truncated, info = env.step(action)

        # Affichage des r√©sultats
        if info.get("farkle"):
            print(f"\nüé≤ FARKLE! Perte de {info['lost_points']} points")
        elif info.get("invalid_action"):
            print("\n‚ùå Action invalide!")
        elif info.get("stopped"):
            print(f"\nüõë Tour termin√©! Points gagn√©s: {reward}")
        elif info.get("win"):
            print(f"\nüèÜ Victoire! Points finaux: {reward}")
        else:
            print(f"\n‚úîÔ∏è Points gagn√©s ce coup: {reward}")

    # Affichage des r√©sultats finaux
    print("\nüéÆ Partie termin√©e!")
    print(f"Scores finaux: Joueur 1 = {env.scores[0]}, Joueur 2 = {env.scores[1]}")
    if env.scores[0] > env.scores[1]:
        print("üéâ F√©licitations! Vous avez gagn√©!")
    else:
        print("üòî L'adversaire a gagn√©. Meilleure chance la prochaine fois!")


def play_with_dqn(env, model, predict_func, episodes=100):
    episode_scores = []
    episode_times = []
    episode_steps = []
    step_times = []
    total_time = 0

    for episode in range(episodes):
        env.reset()
        nb_turns = 0

        start_time = time.time()
        while not env.is_game_over() and nb_turns < 100:
            s = env.state_description()
            s_tensor = tf.convert_to_tensor(s, dtype=tf.float32)
            mask = env.action_mask()
            mask_tensor = tf.convert_to_tensor(mask, dtype=tf.float32)

            q_s = predict_func(model, s_tensor)


            a = epsilon_greedy_action(q_s.numpy(), mask_tensor, env.available_actions_ids(), 0.000001)
            if a not in env.available_actions_ids():
                print(f"Action {a} invalide, prise al√©atoire √† la place.")
                a = np.random.choice(env.available_actions_ids())

            env.step(a)
            nb_turns += 1

        end_time = time.time()
        if nb_turns == 100:
            episode_scores.append(-1)
        else:
            episode_scores.append(env.score(testing=True))
            
        episode_time = end_time - start_time
        episode_times.append(episode_time)
        total_time += episode_time
        episode_steps.append(nb_turns)
        step_times.append(episode_time / nb_turns)

    return (
        mean(episode_scores),
        mean(episode_times),
        mean(episode_steps),
        mean(step_times),
        episode_scores.count(1.0) / episodes
    )


def log_metrics_to_dataframe(
        function,
        model,
        predict_func,
        env,
        episode_index,
        games = 100,
        dataframe = None
):
    if dataframe is None:
        dataframe = pd.DataFrame(
            {
                'training_episode_index': pd.Series(dtype='int'),
                'mean_score': pd.Series(dtype='float'),
                'mean_time_per_episode': pd.Series(dtype='float'),
                'win_rate': pd.Series(dtype='float'),
                'mean_steps_per_episode': pd.Series(dtype='float'),
                'mean_time_per_step': pd.Series(dtype='float')
            }
        )

    (
        mean_score,
        mean_time_per_episode,
        mean_steps_per_episode,
        mean_time_per_step,
        win_rate
     ) = function(env, model, predict_func, games)

    print(f"Episode {episode_index}: Mean Score: {mean_score}, Mean Time: {mean_time_per_episode}, Win Rate: {win_rate}, Mean Steps: {mean_steps_per_episode}, Mean Time per Step: {mean_time_per_step}")

    dataframe = pd.concat([
        dataframe,
        pd.DataFrame([{
            'training_episode_index': episode_index,
            'mean_score': mean_score,
            'mean_time_per_episode': mean_time_per_episode,
            'win_rate': win_rate,
            'mean_steps_per_episode': mean_steps_per_episode,
            'mean_time_per_step': mean_time_per_step
        }])
    ], ignore_index=True)

    return dataframe

def plot_csv_data(
        file_path,
        model = None,
        title = "Training Metrics",
        custom_dict = None,
        algo_name = "",
        env_descr = ""
):
    """
    Lit les donn√©es d'un fichier CSV et cr√©e des graphiques pour analyser les performances d'entra√Ænement.

    Arguments:
        file_path (str): Le chemin du fichier CSV.
        model (tf.keras.Model): Le mod√®le utilis√© pour l'entra√Ænement.
        title (str): Le titre global du graphique.
        custom_dict (dict): Un dictionnaire de param√®tres personnalis√©s √† afficher dans le graphique.
        algo_name (str): Le nom de l'algorithme utilis√©.
        env_descr (str): La description de l'environnement utilis√©.
    """
    data = pd.read_csv(file_path)

    x = data['training_episode_index']
    metrics = {
        'Mean Score': data['mean_score'],
        'Mean Time per Episode': data['mean_time_per_episode'],
        'Win Rate': data['win_rate'],
        'Mean Steps per Episode': data['mean_steps_per_episode'],
        'Mean Time per Step': data['mean_time_per_step']
    }

    plt.figure(figsize=(20, 20))
    plt.suptitle(title, fontsize=20)

    plt.subplot(4, 2, 1)
    plt.axis('off')
    plt.text(
        0.5, 0.8, f"algo: {algo_name}",
        fontsize=18,
        ha='center', va='center', wrap=True
    )
    plt.text(
        0.5, 0.6, f"env: {env_descr}",
        fontsize=18,
        ha='center', va='center', wrap=True
    )
    plt.text(
        0.5, 0.2, str(custom_dict),
        fontsize=17,
        ha='center', va='center', wrap=True
    )


    if model is not None:
        model_summary = io.StringIO()
        model.summary(print_fn=lambda x: model_summary.write(x + '\n'))

        plt.subplot(4, 2, 2)
        plt.axis('off')
        plt.text(
            0.5, 0.5, model_summary.getvalue(),
            fontsize=11,
            ha='center', va='center', wrap=True
        )

    for i, (label, y) in enumerate(metrics.items()):
        plt.subplot(4, 2, i + 3)
        plt.plot(x, y, marker="o")
        plt.title(label)
        plt.xlabel('Training Episode Index')
        plt.ylabel(label)
        plt.grid(True)

    plt.subplots_adjust(hspace=0.4)
    plt.tight_layout()
    plt.savefig(f'{file_path}.png')
    plt.show()
    plt.close()

def play_with_reinforce(env, model, predict_func = None, episodes = 100):
    episode_scores = []
    episode_times = []
    episode_steps = []
    step_times = []
    total_time = 0

    for episode in range(episodes):
        env.reset()
        nb_turns = 0

        start_time = time.time()
        while not env.is_game_over() and nb_turns < 100:
            state = env.state_description()
            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
            valid_actions = env.available_actions_ids()

            probs = model(tf.expand_dims(state_tensor, 0), training=False)[0]
            mask = np.ones_like(probs.numpy()) * float('-inf')
            mask[valid_actions] = 0
            masked_probs = tf.nn.softmax(probs + mask).numpy()

            # En √©valuation, on prend l'action la plus probable
            if len(valid_actions) > 0:
                action = valid_actions[np.argmax(masked_probs[valid_actions])]
            else:
                print("Aucune action valide disponible!")
                action = np.random.choice(env.available_actions_ids())

            env.step(action)
            nb_turns += 1

        end_time = time.time()
        if nb_turns == 100:
            episode_scores.append(-1)
        else:
            episode_scores.append(env.score())

        episode_time = end_time - start_time
        episode_times.append(episode_time)
        total_time += episode_time
        episode_steps.append(nb_turns)
        step_times.append(0 if nb_turns == 0 else episode_time / nb_turns)


    return (
        mean(episode_scores),
        mean(episode_times),
        mean(episode_steps),
        mean(step_times),
        episode_scores.count(1.0) / episodes
    )


def play_with_mcts(env, agent, episodes=100):
    """Fonction pour jouer plusieurs √©pisodes et collecter les statistiques."""
    episode_scores = []
    episode_times = []
    episode_steps = []
    step_times = []
    total_time = 0

    for episode in range(episodes):
        env.reset()
        nb_turns = 0

        start_time = time.time()
        while not env.is_game_over() and nb_turns < 100:
            # Gestion des tours pour Farkle

            action = agent.select_action(env)
            env.step(action)
            env.display()
            nb_turns += 1

        end_time = time.time()

        # Enregistrement des statistiques
        if nb_turns == 100:  # Limite de tours atteinte
            episode_scores.append(-1)
        else:
            episode_scores.append(env.score())

        episode_time = end_time - start_time
        episode_times.append(episode_time)
        total_time += episode_time
        episode_steps.append(nb_turns)
        step_times.append(episode_time / nb_turns if nb_turns > 0 else 0)

    return (
        mean(episode_scores),
        mean(episode_times),
        mean(episode_steps),
        mean(step_times),
        episode_scores.count(1.0) / episodes
    )


def play_with_reinforce_critic(env, model, predict_func=None, episodes=100):
    """Fonction d'√©valuation standard"""
    episode_scores = []
    episode_times = []
    episode_steps = []
    step_times = []
    total_time = 0

    for episode in range(episodes):
        env.reset()
        nb_turns = 0
        start_time = time.time()
        state = env.state_description()
        done = False
        total_reward = 0

        while not done and nb_turns < 100:
            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
            valid_actions = env.available_actions_ids()
            action_mask = env.action_mask()

            # En √©valuation, on prend l'action la plus probable
            logits = model(state_tensor[None])[0].numpy()
            mask = np.ones_like(logits) * float('-inf')
            mask[valid_actions] = 0
            masked_probs = tf.nn.softmax(logits + mask).numpy()
            action = valid_actions[np.argmax(masked_probs[valid_actions])]

            env.step(action)
            reward = env.score(testing=True)
            done = env.is_game_over()
            state = env.state_description()
            total_reward += reward
            nb_turns += 1

        end_time = time.time()
        episode_time = end_time - start_time

        episode_scores.append(total_reward)
        episode_times.append(episode_time)
        episode_steps.append(nb_turns)
        step_times.append(episode_time / nb_turns if nb_turns > 0 else 0)

    return (
        mean(episode_scores),
        mean(episode_times),
        mean(episode_steps),
        mean(step_times),
        len([s for s in episode_scores if s > 0]) / episodes
    )


def play_with_ppo(env, model, predict_func=None, episodes=100):
    """Fonction d'√©valuation standardis√©e"""
    episode_scores = []
    episode_times = []
    episode_steps = []
    step_times = []
    total_time = 0

    for episode in range(episodes):
        env.reset()
        nb_turns = 0
        start_time = time.time()
        state = env.state_description()
        done = False
        episode_reward = 0

        while not done and nb_turns < 100:
            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
            action_mask = env.action_mask()
            valid_actions = env.available_actions_ids()

            # En √©valuation, on prend l'action avec la plus haute probabilit√©
            probs = model(state_tensor[None])[0].numpy()
            mask = np.ones_like(probs) * float('-inf')
            mask[valid_actions] = 0
            masked_probs = tf.nn.softmax(probs + mask).numpy()

            action = valid_actions[np.argmax(masked_probs[valid_actions])]

            env.step(action)
            reward = env.score()
            done = env.is_game_over()
            state = env.state_description()
            episode_reward += reward
            nb_turns += 1

        end_time = time.time()
        episode_scores.append(episode_reward)
        episode_time = end_time - start_time
        episode_times.append(episode_time)
        episode_steps.append(nb_turns)
        step_times.append(episode_time / nb_turns if nb_turns > 0 else 0)

    return (
        mean(episode_scores),
        mean(episode_times),
        mean(episode_steps),
        mean(step_times),
        len([s for s in episode_scores if s > 0]) / episodes
    )


def log_metrics_to_dataframe_mcts(function, agent, env, episode_index, games=100, dataframe=None):
    """Enregistre les m√©triques dans un DataFrame."""
    if dataframe is None:
        dataframe = pd.DataFrame({
            'training_episode_index': pd.Series(dtype='int'),
            'mean_score': pd.Series(dtype='float'),
            'mean_time_per_episode': pd.Series(dtype='float'),
            'win_rate': pd.Series(dtype='float'),
            'mean_steps_per_episode': pd.Series(dtype='float'),
            'mean_time_per_step': pd.Series(dtype='float')
        })

    (
        mean_score,
        mean_time_per_episode,
        mean_steps_per_episode,
        mean_time_per_step,
        win_rate
    ) = function(env, agent, games)

    print(f"Episode {episode_index}: Mean Score: {mean_score:.3f}, Mean Time: {mean_time_per_episode:.3f}, "
          f"Win Rate: {win_rate:.2%}, Mean Steps: {mean_steps_per_episode:.1f}, "
          f"Mean Time per Step: {mean_time_per_step:.3f}")

    dataframe = pd.concat([
        dataframe,
        pd.DataFrame([{
            'training_episode_index': episode_index,
            'mean_score': mean_score,
            'mean_time_per_episode': mean_time_per_episode,
            'win_rate': win_rate,
            'mean_steps_per_episode': mean_steps_per_episode,
            'mean_time_per_step': mean_time_per_step
        }])
    ], ignore_index=True)

    return dataframe


def plot_mcts_metrics(file_path, title="MCTS Training Metrics", agent=None, custom_dict=None):
    """Plot les m√©triques d'entra√Ænement."""
    data = pd.read_csv(file_path)

    x = data['training_episode_index']
    metrics = {
        'Mean Score': data['mean_score'],
        'Mean Time per Episode': data['mean_time_per_episode'],
        'Win Rate': data['win_rate'],
        'Mean Steps per Episode': data['mean_steps_per_episode'],
        'Mean Time per Step': data['mean_time_per_step']
    }

    # Cr√©ation du plot avec matplotlib
    plt.figure(figsize=(15, 10))
    for i, (metric_name, metric_data) in enumerate(metrics.items(), 1):
        plt.subplot(2, 3, i)
        plt.plot(x, metric_data)
        plt.title(metric_name)
        plt.xlabel('Training Episode')
        plt.grid(True)

    plt.tight_layout()
    plt.suptitle(title, y=1.02, fontsize=16)
    plt.show()


def play_agent_vs_random_tictactoe(env, model, num_games: int = 100) -> None:
    try:
        wins = 0
        losses = 0
        draws = 0

        print(Fore.CYAN + f"\n=== Agent vs Random sur {num_games} parties ===")

        for game in range(num_games):
            env.reset()
            game_done = False

            while not game_done:
                # Tour de l'agent
                state = env.state_description()
                s_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
                mask = env.action_mask()

                q_s = dqn_model_predict(model, s_tensor)
                print(q_s,mask,s_tensor)

                masked_q_values = q_s[0].numpy() * mask - 1e9 * (1 - mask)
                action = np.argmax(masked_q_values)

                reward = env.step(action)
                env.display()
                game_done = env.is_game_over()

                if game_done:
                    break

            # R√©sultat de la partie
            if env.score() > 0:
                wins += 1
            elif env.score() < 0:
                losses += 1
            else:
                draws += 1

            if (game + 1) % 10 == 0:
                print(Fore.GREEN + f"\rParties jou√©es : {game + 1}/{num_games}", end="")

        print(Fore.GREEN + "\n\nR√©sultats :")
        print(f"Victoires : {wins} ({wins / num_games * 100:.1f}%)")
        print(f"D√©faites : {losses} ({losses / num_games * 100:.1f}%)")
        print(f"Nuls : {draws} ({draws / num_games * 100:.1f}%)")

    except Exception as e:
        print(Fore.RED + f"\nErreur lors du chargement du mod√®le : {e}")
        print(Fore.YELLOW + "Le mod√®le n'a pas pu √™tre charg√©")


def play_with_agent_gridworld(env, model, num_games: int = 100) -> None:
    try:
        wins = 0
        losses = 0
        draws = 0

        print(Fore.CYAN + f"\n=== Agent vs Random sur {num_games} parties ===")

        for game in range(num_games):
            env.reset()
            game_done = False
            cumulative_reward = 0

            while not game_done:
                state = env.state_description()
                s_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
                mask = env.action_mask()
                mask_array = np.array(mask, dtype=np.float32)

                q_s = dqn_model_predict(model, s_tensor)
                print(q_s, mask, s_tensor)

                masked_q_values = q_s[0].numpy() * mask_array - 1e9 * (1 - mask_array)
                action = np.argmax(masked_q_values)

                reward = env.step(action)
                env.display()
                game_done = env.is_game_over()

                # Gestion de la r√©compense qui peut √™tre None
                if reward is not None:
                    cumulative_reward += reward

            # √âvaluation de la performance √† la fin de la partie
            if cumulative_reward > 0:
                wins += 1
            elif cumulative_reward < 0:
                losses += 1
            else:
                draws += 1

            if (game + 1) % 10 == 0:
                print(Fore.GREEN + f"\rParties jou√©es : {game + 1}/{num_games}", end="")

        print(Fore.GREEN + "\n\nR√©sultats :")
        print(f"Victoires : {wins} ({wins / num_games * 100:.1f}%)")
        print(f"D√©faites : {losses} ({losses / num_games * 100:.1f}%)")
        print(f"Nuls : {draws} ({draws / num_games * 100:.1f}%)")

    except Exception as e:
        print(Fore.RED + f"\nErreur lors du chargement du mod√®le : {e}")
        print(Fore.YELLOW + "Le mod√®le n'a pas pu √™tre charg√©")


def play_with_agent_lineworld(env, model, num_games: int = 100) -> None:
    try:
        wins = 0
        losses = 0
        draws = 0

        print(f"\n=== Agent vs Random sur {num_games} parties ===")

        for game in range(num_games):
            env.reset()
            game_done = False
            cumulative_reward = 0

            while not game_done:
                state = env.state_description()
                s_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
                env.display()
                mask = env.action_mask()
                print(f"Mask: {mask}")
                print(f"State tensor: {s_tensor}")

                try:
                    q_s = dqn_model_predict(model, s_tensor)
                    print(f"Q-values: {q_s}")
                    masked_q_values = q_s[0].numpy() * mask - 1e9 * (1 - mask)
                    action = np.argmax(masked_q_values)
                    reward = env.step(action)
                    game_done = env.is_game_over()

                    # Accumulation des r√©compenses non-nulles
                    if reward is not None:
                        cumulative_reward += reward
                except Exception as e:
                    print(f"Erreur lors de la pr√©diction du mod√®le : {e}")
                    break

            # √âvaluation de la performance
            if cumulative_reward > 0:
                wins += 1
            elif cumulative_reward < 0:
                losses += 1
            else:
                draws += 1

            # Affichage de la progression
            if (game + 1) % 10 == 0:
                print(f"\rParties jou√©es : {game + 1}/{num_games} (Victoires: {wins}, Pertes: {losses}, Nuls: {draws})",
                      end="")

        # Statistiques finales
        print("\n\nR√©sultats finaux :")
        print(f"Victoires : {wins} ({wins / num_games * 100:.1f}%)")
        print(f"D√©faites : {losses} ({losses / num_games * 100:.1f}%)")
        print(f"Nuls : {draws} ({draws / num_games * 100:.1f}%)")
        print(f"Score moyen par partie : {cumulative_reward / num_games:.2f}")

    except Exception as e:
        print(f"\nErreur lors du chargement du mod√®le : {e}")
        print("Le mod√®le n'a pas pu √™tre charg√©")
